---
title: "Design and Analysis of Computer Experiments"
author: "Jason Hilton and Jakub Bijak, University of Southampton"
date: "Tuesday, October 28, 2014"
output: html_document
---

<!---
Design and Analysis of Computer Experiments
========================================================



**Jason Hilton and Jakub Bijak** - University of Southampton 
-->

Workshop given for IDEM 112 'Agent-based modelling and simulation'  
Part of the MPIDR [International Advanced Studies in Demography series](http://www.demogr.mpg.de/en/education_career/international_advanced_studies_in_demography_3279/default.htm)

Location: Max Planck Institute for Demographic Research, Rostock  
Date: 30<sup>th<\sup> October 2014

***

#Introduction 

In this workshop, we shall work through examples of a number of the techniques discussed in the preceding lectures. 

All files and supporting information are available on the github page [https://github.com/jasonhilton/Comp_Exp_Workshop](https://github.com/jasonhilton/Comp_Exp_Workshop).

I shall try to keep the R code is clear as possible throughout, and include comments and explanations in the text, but remember that you can use the '?' command to access the R help for any command if necessary.
Try this for the *lapply* is you are not already familiar with it. 

This workshop leans heavily on ideas from  
Managing Uncertainty in Complex Models Toolkit (2011), MUCM project, [http://mucm.aston.ac.uk/toolkit/index.php?page=MetaHomePage.html](http://mucm.aston.ac.uk/toolkit/index.php?page=MetaHomePage.html) 

Santer, W., Williams, B, and Notz, W (2003)  *'Design and Analysis of Computer Experiements'*, Springer  


We are also using fewer simulation runs than we might generally wish for. This is largely because we want you to spend more time trying out the various methods, and less time waiting for simulations to finish running!

#Part 1: Experimental Designs and Simple Metamodels  

## A First Experiment  
We will start by running some very simple experiments to examine some of the ideas discussed in the lecture. 

We will run netlogo through R using the RNetLogo Library, which you are all familiar with. 
You will need to edit the 'nl.path' variable to point to the folder where netlogo is installed on your machine.

Notice the 'gui' option has been set to false throughout this workshop, as running in 'headless' mode results in quicker runs. The gui is great for development, debugging and demonstration, but not necessarily much use for 'production' runs (ie, those required to produce your results).

```{r}
library(RNetLogo)
### CHANGE THIS PATH if necessary ###
nl.path<- "C:\\Program Files (x86)\\NetLogo 5.0.4"
NLStart(nl.path, gui=F)

```

Our first experiment subject is Schelling's famous segregation model. Most of you will I expect already be familiar with this model by now, but a brief summary is given below in any case.

This examines how individual's moderate preference for living with those similar to themselves can lead to almost complete separation of different types of people. The model aims to show how observed macro-level racial segregation patterns in American cities need not have been caused by explicit racism, but may emerged out of weaker micro-level preferences.

This is one of the standard NetLogo models, so we can load it from the model library as below. 

```{r}
model.path <- "/models/Sample Models/Social Science/Segregation.nlogo"
NLLoadModel(paste(nl.path,model.path,sep=""))
```

Recall, we are interested in how our model **inputs** map to outputs or **responses**.
In this case we have two main inputs - the micro-level preference for similar agents, and the total number of agents present. Given the fixed grid size of $51*51$ patches, this latter input can also be thought of as the population density of the area in question. The output is the average proportion of similar neighbours over all agents - a proxy for segregation. 

Let's run the simulation at one combination of inputs and print the output to the screen.
```{r}
NLCommand("set %-similar-wanted 50")
NLCommand("set number 1500")
NLCommand("setup")
NLDoCommand(100,"go")
NLReport("percent-similar")
```

Here we see that for agents desiring at least half of their neighbours to be similar to themselves, together with a population density of $\frac{1500}{51^{2}} =$ `r 1500/51**2`, the average proportion of similar agents in a neighbourhood is around about 90%. 

##Exploring the Parameter Space

We want to examine how this response varies over the parameter space. An obvious - though not necessarily optimal - place to start is to hold one input steady while varying the other. 

```{r}
# Our desired inputs - a sequence from 0 to 100 increasing by 10 for similar
similar_desired_range <- seq(0,100,10)
number<-1500

runModel<-function(similar,num){
  # function running the model for 50 ticks at inputs 'similar' and 'num' 
  # returning the global proportion similar
  NLCommand("set %-similar-wanted", similar)
  NLCommand("set number", num)
  NLCommand("setup")
  NLDoCommand(50,"go")
  return(NLReport("percent-similar"))
}

# Apply the function runModel to each value in similar_desired, 
# holding number of agents constant at 'number', returning results as an array.
global_similar<-sapply(similar_desired_range, runModel, num=number)

# plot the results 
plot(similar_desired_range, global_similar, 
     main=paste("Response by values of '%-similar-desired',", number, "agents"))

```

By observation, it seems that segregation increases with micro-level preference for similar neighbours up to a threshold of about 70-80% desired similar neighbours, at which point there is a sharp decrease to the 50%. Why might this be the case?

Similarly, we can hold '%-similar-desired' steady, and vary only the number of agents in the simulation ( and by extension the population density)

```{r}
  similar_desired <- 50
  number_range<-seq(500,2500,250)
  global_similar2<-sapply(X=number_range, runModel, similar = similar_desired)
  plot(number_range, global_similar2, 
       main = paste("Response by number of agents. %-similar-wanted = ", similar_desired ))
```

Here it seems as though increasing the number of agents decreases segregation, although note the scale on the y-axis.

By holding the one parameter fixed while varying the other, we are preventing ourselves from identifying any interaction between the variables, and leaving large areas of the parameter space unobserved. 

We can see this by simply plotting our design:

```{r}

# here I simply combine all inputs in a single data frame. 
design<-data.frame(similar_desired=c(similar_desired_range,
                                   rep(similar_desired, length(number_range))),
                  number=c(rep(number,length(similar_desired_range)), 
                                     number_range))
plot(design)

```

To examine the corners of the parameter space, and to attempt to capture interactions between the variables, we will now run our simulation on a full factorial design. 
We will use 5 levels (this may take a few moments to run)  

```{r}
fact_design<-expand.grid(similar_desired=seq(0,100,25), number=seq(500,2500,500))

plot(fact_design)

fact_response<-mapply(runModel, fact_design$similar_desired, fact_design$number)

```

We can plot this as a surface using the persp command:

```{r}
persp(unique(fact_design$similar_desired),
      unique(fact_design$number), 
      matrix(fact_response,nrow=5),
      xlab="Similar Desired",
      ylab="Number of Agents",
      zlab="Response",
      theta=220,
      phi=30,
      shade=0.6,
      col="lightblue"
      )
```

This looks nice, but generally a contour plot is easier to interpret, and requires
less tuning to find a good viewing angle. 

```{r}
filled.contour(unique(fact_design$similar_desired),
      unique(fact_design$number), 
      matrix(fact_response,nrow=5),
      xlab="Similar Desired",
      ylab="Number of Agents",
      main="Average % Neighbour Similar by % similar desired and number of agents",
      cex.main=0.9)
```

With both these methods, we have to be carefull to realise that the plot algorithms are interpolating between the points we have observed using a simple (meta)model. Also note that the surface is pretty uneven. This is because we are dealling with a *stochastic simulator*, so the unevenness is likely to be attributible to random noise from NetLogo's random number generator. We will talk more about randomness in the part 2 of this workshop. 


# Response surfaces
Now we can start fitting some simple meta-models.
Often it is preferable to standardise the input space so that we can easily compare the effect of different inputs through their regression coefficients.

```{r}
transformInput<-function(input, location, multiplier){
  transformed_input<-input-location
  return(transformed_input/multiplier)
}


locations<-apply(fact_design,2,min)
multipliers<-apply(fact_design,2,max)- locations

trans_design<-data.frame(mapply(transformInput, fact_design, locations, multipliers))

# for convenience, lets add our outputs as a column in the same data frame
trans_design$response<-fact_response

```

We will start by fitting just a main effect to each input

```{r simple lm}
model1<-with(trans_design, lm(response ~ similar_desired + number))
summary(model1)

```

This is not a good fit to the data, which is unsurprising given we observed significant curvature in our reponse surface, which can not be captured by linear terms in our model
Problems with this model can clearly be identified by plotting the standardised residuals against the similar desired input - we would hope that there is no pattern to these residuals.

```{r}
plot(trans_design$similar_desired, rstandard(model1))

``` 

Let's try adding higher order terms, and an interaction: 

```{r}
model2<- with(trans_design, 
              lm( response ~ similar_desired * number + I (number**2) 
                  + I(similar_desired**2)))
summary(model2)

```
This model is a much better fit to the data, as can be seen from the r<sup>2</sup> coefficients.
The residuals are still not perfect, but are considerably better than before.

```{r}
plot(trans_design$similar_desired, rstandard(model2))
plot(trans_design$number, rstandard(model2))
qqnorm(rstandard(model2))
qqline(rstandard(model2))
``` 


## Validation
If we are to trust the results of meta-model, we need to confirm that it is a good fit for the underlying simulation. The best way of doing this is to run the simulation at new points, and compare these to the model predictions. 

```{r}
new_points <- data.frame(similar_desired=runif(10,0,100), number=runif(10,500,2500))
valid_response <- mapply(runModel, new_points$similar_desired, new_points$number)


trans_new_points <- data.frame(mapply(transformInput, 
                                    new_points, locations, multipliers))
model_predictions <- predict(model2, trans_new_points)

error <- valid_response-model_predictions

# Root Mean Squared Error
RMSE <- sqrt(sum(error**2)/length(error))

# 'Normalised' RMSE
RMSE/diff(range(trans_design$response))


```

The normalised RMSE is the probably too high for this model. This is a metric for assessing the error of a prediction based on the root of the average squared error, divided by the range over which the training set response varies. Thus it can be considered a sort of percentage error.  

This might be because of the low number of data-points, but more likely it is due to the inadequacy of model we fit. ABM simulation are often too non-linear to expect simple response surfaces such as the one fitted above to do a good job of 'standing in' for the simulation. However, fitting a response surface does help us to understand the way in which the model responds to different inputs.


The inadequacy of the model in prediction helps to motivate the non-parameteric methods such as kriging used in the next part of the workshop, where no specific functional form is assumed for the meta-model.


## Sensitivity Analysis

We can crudely examine the sensitivity of the model to inputs by considering the
ANOVA decomposition of our model, and comparing the ratios of effect-specific sum of squares to the total:
```{r}
model_an<-anova(model2)

model_an["Sum Sq"]/sum(model_an["Sum Sq"])

```
This indicates that the squared effect of similar-desired is the most significant in our model. This fits with our eyeball intuition, noting the curved nature of the response surface. 

## Other designs
The R package 'lhs' allows easy generation of the LHS sample designs. 
Try using this package and the improvedLHS command to generate an LHS design for the schelling model, and fitting a simple metamodel  to the data. 

What happens to the RMSE for the lhs vs the factorial design, using the same validation points, using the same model and the same number of points?

```{r}
library(lhs)
scaledDesign<-data.frame(improvedLHS(25,2))
head(scaledDesign)

colnames(scaledDesign)<-c("similar_desired", "number")

lhs_design <- mapply(function(x,multiplier,location) (x*multiplier) + location, 
       scaledDesign,multipliers, locations  )

plot(lhs_design)

colnames(lhs_design)<-c("similar_desired", "number")

lhs_design<-data.frame(lhs_design)

lhs_response<-mapply(runModel, lhs_design$similar_desired, lhs_design$number)

scaledDesign$response<-lhs_response

model_lhs1<-with(scaledDesign, lm(response~similar_desired+ number))

model_lhs2<- with(scaledDesign, 
              lm( response ~ similar_desired * number + I (number**2) 
                  + I(similar_desired**2)))



model_predictions_lhs <- predict(model_lhs2, trans_new_points)

error <- valid_response-model_predictions_lhs

# Root Mean Squared Error
RMSE_lhs <- sqrt(sum(error**2)/length(error))

# 'Normalised' RMSE
RMSE_lhs/diff(range(trans_design$response))

RMSE/diff(range(trans_design$response))

```

## Note on the purpose of meta-models 

It might appear that meta-models don't add much to simple plotting of outputs.
For the example we have used here, this may be the case. We have studied a simple and above all low-dimension problem. 

Once you include more parameters, it becomes more and more difficult to identify interactions by eye, and model-based solutions become imperative. 


#Part 2: Uncertainty quantification and Kriging-based Emulation

In the first part of this workshop we have paid very little attention to the uncertainty inherent in simulation. We will start by examining how Monte Carlo techniques can be used to assess the effect of uncertainty. 

## Assessing uncertainty using Monte Carlo
We will start by working with the forest fire model, again from the netlogo model library. This model display displays a lot of stochasticity in outputs, and so is a good target for uncertainty analysis. 

It models the spread of fire through a forest. It has only one parameter - the density of the trees in the forest in question. The simulation introduces fire to the edge of the fire, and lets it spread to any nearby trees. We are examining the proportion of burnt trees as an output. 


```{r}
model.path <- "/models/Sample Models/Earth Science/Fire.nlogo"
NLLoadModel(paste(nl.path,model.path,sep=""))
```

As a test, let's run the simulation a few times at density 60. Note the considerable differences in outputs. 

```{r}
for (i in 1:4){
  NLCommand("set density 60")
  NLCommand("setup")
  NLDoCommand(1000,"go")
  initial <- NLReport("initial-trees")
  burned  <- NLReport("burned-trees")
  print(burned/initial)
}
```


We need to rewrite our runModel function for the new simulation.

```{r}
runModel<-function(density){
  # function running the model for 1000 ticks at input value 'density'
  # returning the proportion of burned trees
  NLCommand("set density", density)
  NLCommand("setup")
  NLDoCommand(1000,"go")
  return(NLReport("burned-trees / initial-trees"))
}
```

Now lets run the simulation 50 times at the density 60 to start to examine what the output *distribution* looks like at this point. When we are doing this sort of experiment it is useful to check how long each set of runs is going to take - otherwise we might be waiting for simulations that will never end!  

The function'*system.time*' is our friend in this context - look at the third column in the output, "elapsed time" to assess the time taken. When working on this yourself, try this with a small number of runs and mutliply up to get an idea of how long it should take for the desired number.

```{r}
system.time( proportion_burned_60<-sapply(rep(60,50),runModel ))
hist(proportion_burned_60)
```

Notice how the results are spread over almost all the whole of the output space, with most of the density concentrated at the right of the distribution. Ideally we want to consider how this distribution changes across 

As we are going to need to repeat our simulation a number of times,  it may be worth harnessing the advantages of parallel computing to speed the process up. 
If your computer has multiple cores, with a bit of setup you can use parallel versions of the apply family of functions to split the work over a number of processors.

```{r}
NLQuit()

library("parallel")

# set number of cores to be used

detectCores()
n_processors<-detectCores()-1
# I tend to leave one core free, hence the -1, but you may not wish to do this.


#create a cluster object in R
cl<-makeCluster(n_processors)

# set up the random number generators
clusterSetRNGStream(cl)

# create a function to set up the model on each core
setupModel<-function(nl.path,model.path){
  library(RNetLogo)
  NLStart(nl.path,gui=F)
  NLLoadModel(paste(nl.path,model.path,sep="")) 
}

# use the parallel version of sapply to run the function on each core
invisible(parSapply(cl, rep(nl.path, n_processors), setupModel, model.path))

```


Now that we have a parallel cluster set up, let's try a few more runs:

```{r}
# run model using parSapply, and combine results with those from before.

system.time(
  proportion_burned_60_par<-parSapply(cl, rep(60,100), runModel)
  )

proportion_burned_60<-c( proportion_burned_60,  proportion_burned_60_par)

```
Hopefully you should notice that this took less time per run than before! We did twice as many runs this time, so we would hope the elapsed time is considerably less than twice the previous value. If not this is probably due to the overhead associated with sending information to different cores and putting it back together again. The more runs you are hoping to do, the more likely it is that you will be able to see benefits from parallelising your model runs.

Note that if you are running this on a Mac or a LINUX based system such as Ubuntu, then you might be able to use the much simpler mclapply function (which, in my experience, is also faster).  

Lets plot our outputs. We can also use the 'density' command to plot the smoothed continuous density of our observations.

```{r}
hist(proportion_burned_60,breaks=15)
plot(density(proportion_burned_60))

```

We now have 150 runs, and we can see from the above histogram that the distribution is pretty irregular. Clearly, a normality assumption will not be appropriate in this case. 

Try and build up a picture of the uncertainty caused by simulation stochasticity across the following values of density by running repeat trials at each value.
There is no need to do 150 runs at each point given time constraints, however - try around 25.

```{r}
densities<-seq(57,81,6)
```


Next, let's assume we have some prior information about the density of a particular forest we are interested in. We want know, given our information, expressed as a *distribution* over possible density values, what the probability of various proportions of forest destruction is. 

Our prior states that the density of the forest has a normal distribution with mean .56 and sd 0.05. Sample from this distribution using rnorm, and run the simulation at the sampled points. Plot the density of this output distribution. 

Now try the same exercise, but just hold the input steady at it's mean value, and examine the difference between the two distributions. 

This is an example of incorporating the effect of *input uncertainty* into our output predictions. (Note this is NOT a case of Full Bayesian updating - we have not applied bayes rule to get a posterior, but used the prior and the model to induce a distribution on our output variable).

```{r}
# tidy up by closing NetLogo on each core, and stopping the cluster.
clusterEvalQ(cl,NLQuit())
stopCluster(cl)

```



# Kriging or Gaussian Processes

## Kriging Models with DiceKriging

Here we will go back to our schelling model and fit two different kriging models. 

We will start by examining the DiceKriging package, which you will need to install from CRAN. This package uses the km command to create a kriging model object in the same way as the lm linear model command.

Use the LHS sample and associated response you created earlier to fit the model.

```{r}
lhs_design$response<-lhs_response
library("DiceKriging")

kriging_m1<-km(response~similar_desired + number, lhs_design[,c(1,2)], lhs_design[,3], nugget.estim=T)

kriging_m1
```

The dice kriging command fits a kriging model, so that data point estimates are the sum of a linear trend and deviations from this trend drawn from a gaussian process.

The trend coefficients are the first elements of the output, and include and intercept and a linear term for each parameter. The output also tells us that the a matern5_2 kernel was used - which defines the shape of the covariance function. Other kernels are available - experiment with them if you wish.  It also gives us the estimates of the correllation parameters (here 'theta'), and the variance and nugget effects. 

The nugget effect here accounts for simulation stochasticity, while the variance parameter refers to input related variance. 

We can make predictions for new values using the predict command. Lets make predictions across the whole parameter space. 

```{r} 
similar_ins<-seq(0,100,2)
number_ins<-seq(500,2500,50)

fullSpace<- data.frame(expand.grid(similar_ins, number_ins))
dim(fullSpace)
#lots of points!

colnames(fullSpace)<-colnames(lhs_design[,1:2])


predictions<-predict(kriging_m1, fullSpace, "UK")

prediction.surface<-matrix(predictions$mean, 51)

filled.contour(similar_ins, number_ins, prediction.surface )

```

Try constructing the same plot for predictions from the quadratic model.
What do you notice? 


## Gaussian Process Models with GEM-SA
We can also try fitting the same model in the software GEM-SA.
This is available as a stand-alone application here: [http://ctcd.group.shef.ac.uk/gem.html](http://ctcd.group.shef.ac.uk/gem.html)

To use the software, we first of all need to save our simulation inputs and results in a format that can be read by the simulation .



```{r}
#set your working directory somewhere sensible so you can find these files later!
write.table(lhs_design[,1:2], file="inputs.txt",row.names=F, col.names=F)
write.table(lhs_design[,3], file="outputs.txt",row.names=F, col.names=F)

```

Now open GEM-SA. 

1. From the menu, choose the 'Project / new ' option.  

2. Next, in the resulting dialogue box, pick the inputs file you have just saved from the browse menu.  

3. Similarly, browse to your outputs file. Leave predictions blank for the moment.  

4. Now, click on the options tab. We have two inputs, so input '2' in the number of inputs box. You can name them appropriately if you wish by pressing the 'Names' button.     

5. Tick the 'code has numerical error' box. In our language, this means that we have a simulation stochasticity, and we want GEM-SA to estimate a nugget term.  

6. Under the Input uncertainty options, check the All unknown, product normal option.
This states that we do not know for certain the true values of the parameters in questions, but we expect them to be jointly normal.  

7. Don't worry about the simulations tab - this refers only to the MCMC parameter estimation procedure.  

8. Press OK. You will be prompted to enter prior means and variance for your input variables. Press the 'use default' button to set use empirical estimated parameters based on the input values we have given GEM-SA.  

9. We're ready to go. Press the red play button at the top of the window!  

#Output Analysis
You should be able to see some plots of the main effects for our two variables. 
If you go to the sensitivity analysis tab, you will a variance based sensitivity analysis - this takes into account the non-parametric deviations from the linear trend that were unable to capture with our ANOVA earlier in the workshop.   

In the 'Output summary' tab, you should be able to see the variance (sigma-squared) and nugget parameters, and roughness parameters which we also estimated using DiceKriging.  


At the bottom of the screen, you can also see the mean expected of the simulation output **given the distributions of our inputs**, and the variance of this expectation.
The final value is the total expected variance of the output, again given our input priors.   


Try changing some of the other options (click on the pen to get back to the options screen). In particular, ask GEM-SA to calculate joint effects, and also attempt to validate the emulator accuracy using leave one out validation - the root mean standardised error should ideally be less than two.  


One can also try examining how changes in the prior distributions impact upon the sensitivity and uncertainty analyses. Does this fit with your expectations?   






# Appendix: Reading and Software

A number of R packages might be able to help you in designing and analysing computer experiments.

##R packages
1. 'lhs':  Provides simple functions to calculate latin hypercube samples. Note that optimumLHS can sometimes be expensive in high dimensions, so the other options are general best. Augment is also a useful function if you need to add more points eg for crossvalidation
2. 'DiceKriging': excellent non-bayesian computater experiment package
3. 'tgp':  a bit more complex. Fits 'treed' gaussian process
4. 'BACCO': implementation of Kennedy and O'Hagan's emulator framework. Deterministic models only, so not always useful
5. 'AlgDesign' :  For generating fraction factorials
6. 'rsm' : Response surface methodology package.

##Other Software
1. Gaussian Process Matlab packages : Algorithms relating to the Rasmussen and Williams excellent book on Gaussian Processes for Machine learning. Both book and software available free online here:
[http://www.gaussianprocess.org/](http://www.gaussianprocess.org/)
Cutting edge. 
2. GEM-SA :  Marc Kennedy's stand-alone gui for Gaussian Processes. Works very well, but is opaque and difficult to integrate with R. 

